#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ—á–Ω–æ–π –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
–°–æ–∑–¥–∞–µ—Ç: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∑–∞–ø—Ä–æ—Å—ã –∏ –∫–ª–∏–∫–∏
"""
import random
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ backend
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from faker import Faker
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import psycopg2

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
fake = Faker(['ru_RU'])
Faker.seed(42)
random.seed(42)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
NUM_USERS = 80
NUM_DOCUMENTS = 600
NUM_QUERIES = 150
CLICKS_PER_QUERY = 15  # –í —Å—Ä–µ–¥–Ω–µ–º

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ë–î
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'library_search',
    'user': 'library_user',
    'password': 'library_password'
}

# ==================== –°–ü–†–ê–í–û–ß–ù–ò–ö–ò ====================

SUBJECTS = [
    'computer_science', 'mathematics', 'physics', 'chemistry',
    'biology', 'economics', 'psychology', 'linguistics',
    'philosophy', 'history', 'sociology', 'law'
]

SUBJECTS_RU = {
    'computer_science': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞',
    'mathematics': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞',
    'physics': '–§–∏–∑–∏–∫–∞',
    'chemistry': '–•–∏–º–∏—è',
    'biology': '–ë–∏–æ–ª–æ–≥–∏—è',
    'economics': '–≠–∫–æ–Ω–æ–º–∏–∫–∞',
    'psychology': '–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è',
    'linguistics': '–õ–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞',
    'philosophy': '–§–∏–ª–æ—Å–æ—Ñ–∏—è',
    'history': '–ò—Å—Ç–æ—Ä–∏—è',
    'sociology': '–°–æ—Ü–∏–æ–ª–æ–≥–∏—è',
    'law': '–Æ—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏—è'
}

DOCUMENT_TYPES = ['textbook', 'article', 'monograph']

USER_ROLES = ['student', 'master', 'phd', 'professor']

# –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º
SUBJECT_TERMS = {
    'computer_science': [
        '–∞–ª–≥–æ—Ä–∏—Ç–º—ã', '—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏',
        '–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã', '–∫–æ–º–ø–∏–ª—è—Ç–æ—Ä—ã',
        '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ', '–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞',
        '—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã', '–∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è', '–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞'
    ],
    'mathematics': [
        '–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑', '–ª–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞', '–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è',
        '—Ç–µ–æ—Ä–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π', '–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–¥–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞',
        '—Ç–æ–ø–æ–ª–æ–≥–∏—è', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑', '—Ç–µ–æ—Ä–∏—è —á–∏—Å–µ–ª', '–∫–æ–º–±–∏–Ω–∞—Ç–æ—Ä–∏–∫–∞',
        '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '—á–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã'
    ],
    'physics': [
        '–º–µ—Ö–∞–Ω–∏–∫–∞', '—Ç–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏–∫–∞', '—ç–ª–µ–∫—Ç—Ä–æ–¥–∏–Ω–∞–º–∏–∫–∞', '–∫–≤–∞–Ω—Ç–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞',
        '—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∏–∑–∏–∫–∞', '–æ–ø—Ç–∏–∫–∞', '—è–¥–µ—Ä–Ω–∞—è —Ñ–∏–∑–∏–∫–∞', '—Ñ–∏–∑–∏–∫–∞ —Ç–≤–µ—Ä–¥–æ–≥–æ —Ç–µ–ª–∞',
        '–∞—Å—Ç—Ä–æ—Ñ–∏–∑–∏–∫–∞', '—Ç–µ–æ—Ä–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏', '–∫–≤–∞–Ω—Ç–æ–≤–∞—è —Ç–µ–æ—Ä–∏—è –ø–æ–ª—è'
    ],
    'chemistry': [
        '–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∞—è —Ö–∏–º–∏—è', '–Ω–µ–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∞—è —Ö–∏–º–∏—è', '—Ñ–∏–∑–∏—á–µ—Å–∫–∞—è —Ö–∏–º–∏—è',
        '–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∞—è —Ö–∏–º–∏—è', '–±–∏–æ—Ö–∏–º–∏—è', '–∫–≤–∞–Ω—Ç–æ–≤–∞—è —Ö–∏–º–∏—è', '—Ö–∏–º–∏—á–µ—Å–∫–∞—è –∫–∏–Ω–µ—Ç–∏–∫–∞',
        '—ç–ª–µ–∫—Ç—Ä–æ—Ö–∏–º–∏—è', '–∫–æ–ª–ª–æ–∏–¥–Ω–∞—è —Ö–∏–º–∏—è'
    ],
    'biology': [
        '–º–æ–ª–µ–∫—É–ª—è—Ä–Ω–∞—è –±–∏–æ–ª–æ–≥–∏—è', '–≥–µ–Ω–µ—Ç–∏–∫–∞', '–±–∏–æ—Ö–∏–º–∏—è', '–º–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏—è',
        '—ç–∫–æ–ª–æ–≥–∏—è', '—ç–≤–æ–ª—é—Ü–∏—è', '—Ñ–∏–∑–∏–æ–ª–æ–≥–∏—è', '–∞–Ω–∞—Ç–æ–º–∏—è', '–±–æ—Ç–∞–Ω–∏–∫–∞',
        '–∑–æ–æ–ª–æ–≥–∏—è', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è'
    ],
    'economics': [
        '–º–∏–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞', '–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—ç–∫–æ–Ω–æ–º–µ—Ç—Ä–∏–∫–∞', '—Ñ–∏–Ω–∞–Ω—Å—ã',
        '–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞', '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è',
        '–±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π —É—á–µ—Ç', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'
    ],
    'psychology': [
        '–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è', '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è', '–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è',
        '–∫–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è', '–Ω–µ–π—Ä–æ–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è', '–ø—Å–∏—Ö–æ–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞',
        '–ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–∏—è', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è'
    ],
    'linguistics': [
        '—Ñ–æ–Ω–µ—Ç–∏–∫–∞', '–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è', '—Å–∏–Ω—Ç–∞–∫—Å–∏—Å', '—Å–µ–º–∞–Ω—Ç–∏–∫–∞', '–ø—Ä–∞–≥–º–∞—Ç–∏–∫–∞',
        '—Å–æ—Ü–∏–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞', '–ø—Å–∏—Ö–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞', '—Ç–∏–ø–æ–ª–æ–≥–∏—è —è–∑—ã–∫–æ–≤',
        '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞', '–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞'
    ],
    'philosophy': [
        '–æ–Ω—Ç–æ–ª–æ–≥–∏—è', '–≥–Ω–æ—Å–µ–æ–ª–æ–≥–∏—è', '—ç—Ç–∏–∫–∞', '–ª–æ–≥–∏–∫–∞', '—ç—Å—Ç–µ—Ç–∏–∫–∞',
        '—Ñ–∏–ª–æ—Å–æ—Ñ–∏—è –Ω–∞—É–∫–∏', '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è —Ñ–∏–ª–æ—Å–æ—Ñ–∏—è', '—Ñ–∏–ª–æ—Å–æ—Ñ–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è',
        '—Ñ–µ–Ω–æ–º–µ–Ω–æ–ª–æ–≥–∏—è', '–≥–µ—Ä–º–µ–Ω–µ–≤—Ç–∏–∫–∞'
    ],
    'history': [
        '–¥—Ä–µ–≤–Ω—è—è –∏—Å—Ç–æ—Ä–∏—è', '—Å—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤—å–µ', '–Ω–æ–≤–æ–µ –≤—Ä–µ–º—è', '–Ω–æ–≤–µ–π—à–∞—è –∏—Å—Ç–æ—Ä–∏—è',
        '–∏—Å—Ç–æ—Ä–∏–æ–≥—Ä–∞—Ñ–∏—è', '–∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è', '—ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏—è', '–∏—Å—Ç–æ—Ä–∏—è –†–æ—Å—Å–∏–∏',
        '–≤—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è', '–≤–æ–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è'
    ],
    'sociology': [
        '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è', '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞', '—Å–æ—Ü–∏–æ–ª–æ–≥–∏—è –∫—É–ª—å—Ç—É—Ä—ã',
        '—Å–æ—Ü–∏–æ–ª–æ–≥–∏—è —Ä–µ–ª–∏–≥–∏–∏', '—Å–æ—Ü–∏–æ–ª–æ–≥–∏—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è', '—É—Ä–±–∞–Ω–∏—Å—Ç–∏–∫–∞',
        '—Å–æ—Ü–∏–æ–ª–æ–≥–∏—è —Å–µ–º—å–∏', '—Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è'
    ],
    'law': [
        '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–µ –ø—Ä–∞–≤–æ', '—É–≥–æ–ª–æ–≤–Ω–æ–µ –ø—Ä–∞–≤–æ', '–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–∞–≤–æ',
        '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–æ', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –ø—Ä–∞–≤–æ', '—Ç—Ä—É–¥–æ–≤–æ–µ –ø—Ä–∞–≤–æ',
        '–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–µ –ø—Ä–∞–≤–æ', '—Ç–µ–æ—Ä–∏—è –ø—Ä–∞–≤–∞'
    ]
}


# ==================== –ì–ï–ù–ï–†–ê–¢–û–†–´ ====================

def generate_users(num_users: int) -> List[Dict[str, Any]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    print(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_users} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")

    users = []
    role_distribution = {
        'student': 0.50,  # 50% —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
        'master': 0.25,  # 25% –º–∞–≥–∏—Å—Ç—Ä–∞–Ω—Ç–æ–≤
        'phd': 0.15,  # 15% –∞—Å–ø–∏—Ä–∞–Ω—Ç–æ–≤
        'professor': 0.10  # 10% –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π
    }

    for i in range(num_users):
        role = random.choices(
            list(role_distribution.keys()),
            weights=list(role_distribution.values())
        )[0]

        specialization = random.choice(SUBJECTS)

        # –ò–Ω—Ç–µ—Ä–µ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (2-4 —Ç–µ–º—ã)
        num_interests = random.randint(2, 4)
        interests = random.sample(SUBJECT_TERMS[specialization], num_interests)

        # –ö—É—Ä—Å —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –∏ –º–∞–≥–∏—Å—Ç—Ä–æ–≤
        course = None
        if role == 'student':
            course = random.randint(1, 4)
        elif role == 'master':
            course = random.randint(1, 2)

        username = fake.user_name() + str(random.randint(100, 999))

        user = {
            'username': username,
            'email': f"{username}@university.edu",
            'role': role,
            'specialization': specialization,
            'course': course,
            'interests': interests
        }

        users.append(user)

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    return users


def generate_documents(num_documents: int) -> List[Dict[str, Any]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    print(f"üìö –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

    documents = []

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    type_distribution = {
        'textbook': 0.40,  # 40% —É—á–µ–±–Ω–∏–∫–∏
        'article': 0.35,  # 35% —Å—Ç–∞—Ç—å–∏
        'monograph': 0.25  # 25% –º–æ–Ω–æ–≥—Ä–∞—Ñ–∏–∏
    }

    for i in range(num_documents):
        doc_type = random.choices(
            list(type_distribution.keys()),
            weights=list(type_distribution.values())
        )[0]

        subject = random.choice(SUBJECTS)
        subject_ru = SUBJECTS_RU[subject]
        terms = SUBJECT_TERMS[subject]

        # –í—ã–±–∏—Ä–∞–µ–º 2-3 —Ç–µ—Ä–º–∏–Ω–∞ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è
        title_terms = random.sample(terms, min(random.randint(2, 3), len(terms)))

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if doc_type == 'textbook':
            title = f"{title_terms[0].capitalize()}: —É—á–µ–±–Ω–∏–∫ –ø–æ {subject_ru.lower()}"
        elif doc_type == 'article':
            title = f"{title_terms[0].capitalize()} –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {title_terms[1]}"
        else:  # monograph
            title = f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ {title_terms[0]}: —Ç–µ–æ—Ä–∏—è –∏ –ø—Ä–∞–∫—Ç–∏–∫–∞"

        # –ê–≤—Ç–æ—Ä—ã (1-3)
        num_authors = random.randint(1, 3)
        authors = [fake.name() for _ in range(num_authors)]

        # –ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (2000-2024)
        year = random.randint(2000, 2024)

        # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è
        abstract_terms = random.sample(terms, min(5, len(terms)))
        abstract = (
            f"–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏–∑—É—á–µ–Ω–∏—é {abstract_terms[0]}. "
            f"–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –≤–æ–ø—Ä–æ—Å—ã {abstract_terms[1]} –∏ {abstract_terms[2]}. "
            f"–û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è {abstract_terms[3]}. "
            f"–ú–∞—Ç–µ—Ä–∏–∞–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –≤ –æ–±–ª–∞—Å—Ç–∏ {subject_ru.lower()}."
        )

        # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (—Å–∏–º—É–ª—è—Ü–∏—è)
        content = abstract + "\n\n" + "\n".join([
            f"–ì–ª–∞–≤–∞ {j + 1}. {term.capitalize()}. "
            f"{''.join(fake.sentences(nb=random.randint(3, 5)))}"
            for j, term in enumerate(random.sample(terms, min(5, len(terms))))
        ])

        document = {
            'document_id': f"doc_{i + 1:05d}",
            'title': title,
            'authors': authors,
            'document_type': doc_type,
            'year': year,
            'subject': subject,
            'abstract': abstract,
            'content': content,
            'isbn': fake.isbn13() if doc_type in ['textbook', 'monograph'] else None,
            'doi': f"10.{random.randint(1000, 9999)}/{fake.uuid4()[:8]}" if doc_type == 'article' else None,
            'pages': random.randint(50, 800) if doc_type != 'article' else random.randint(5, 30),
            'language': 'ru'
        }

        documents.append(document)

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return documents


def generate_queries_and_clicks(
        users: List[Dict],
        documents: List[Dict],
        num_queries: int
) -> tuple:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª–∏–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –ø—Ä–æ—Ñ–∏–ª–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    print(f"üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_queries} –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª–∏–∫–æ–≤...")

    queries = []
    clicks = []
    impressions = []

    query_id = 1
    click_id = 1
    impression_id = 1

    for _ in range(num_queries):
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user = random.choice(users)
        user_id = users.index(user) + 1

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if random.random() < 0.7:  # 70% –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–≤—è–∑–∞–Ω—ã —Å –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏
            query_text = random.choice(user['interests'])
        else:  # 30% —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            random_subject = random.choice(SUBJECTS)
            query_text = random.choice(SUBJECT_TERMS[random_subject])

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∫ –∑–∞–ø—Ä–æ—Å—É
        if random.random() < 0.3:
            modifiers = ['–≤–≤–µ–¥–µ–Ω–∏–µ –≤', '–æ—Å–Ω–æ–≤—ã', '–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫—É—Ä—Å', '—Ç–µ–æ—Ä–∏—è']
            query_text = f"{random.choice(modifiers)} {query_text}"

        timestamp = datetime.now() - timedelta(days=random.randint(0, 90))
        session_id = fake.uuid4()

        # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        relevant_docs = [
            doc for doc in documents
            if any(term in doc['title'].lower() or term in doc['abstract'].lower()
                   for term in query_text.lower().split())
        ]

        # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –±–µ—Ä–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if not relevant_docs:
            relevant_docs = [
                doc for doc in documents
                if doc['subject'] == user['specialization']
            ]

        # –ï—Å–ª–∏ –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω–µ—Ç, –±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ
        if not relevant_docs:
            relevant_docs = random.sample(documents, min(20, len(documents)))

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ —Ç–æ–ø-20
        relevant_docs = relevant_docs[:20]

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_count = len(relevant_docs)

        query = {
            'query_id': query_id,
            'user_id': user_id,
            'query_text': query_text,
            'results_count': results_count,
            'timestamp': timestamp,
            'session_id': session_id
        }
        queries.append(query)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∫–∞–∑—ã (impressions) –¥–ª—è —Ç–æ–ø-10
        shown_docs = relevant_docs[:10]
        for position, doc in enumerate(shown_docs, start=1):
            impression = {
                'impression_id': impression_id,
                'query_id': query_id,
                'user_id': user_id,
                'document_id': doc['document_id'],
                'query_text': query_text,
                'position': position,
                'shown_at': timestamp,
                'session_id': session_id
            }
            impressions.append(impression)
            impression_id += 1

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª–∏–∫–∏ (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∏–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–æ—Ñ–∏–ª—è)
        num_clicks = random.randint(0, min(3, len(shown_docs)))

        for _ in range(num_clicks):
            # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∏–∫–∞ –≤—ã—à–µ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π
            position_weights = [1.0 / (i + 1) for i in range(len(shown_docs))]
            clicked_doc = random.choices(shown_docs, weights=position_weights)[0]
            position = shown_docs.index(clicked_doc) + 1

            # –ë—É—Å—Ç –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ—Ñ–∏–ª—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            doc_matches_profile = (
                    clicked_doc['subject'] == user['specialization'] or
                    clicked_doc['document_type'] == ('textbook' if user['role'] == 'student' else 'article')
            )

            if doc_matches_profile:
                if random.random() > 0.3:  # 70% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∏–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    click_time = timestamp + timedelta(seconds=random.randint(1, 30))
                    dwell_time = random.randint(10, 600) if doc_matches_profile else random.randint(5, 60)

                    click = {
                        'click_id': click_id,
                        'query_id': query_id,
                        'user_id': user_id,
                        'document_id': clicked_doc['document_id'],
                        'query_text': query_text,
                        'position': position,
                        'clicked_at': click_time,
                        'dwell_time': dwell_time,
                        'session_id': session_id
                    }
                    clicks.append(click)
                    click_id += 1

        query_id += 1

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(impressions)} –ø–æ–∫–∞–∑–æ–≤")
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(clicks)} –∫–ª–∏–∫–æ–≤")

    return queries, impressions, clicks


# ==================== –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–î ====================

def save_to_database(users, documents, queries, impressions, clicks):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL"""
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL...")

    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        # –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü
        print("üóëÔ∏è  –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        cursor.execute("TRUNCATE TABLE clicks, impressions, search_queries, documents, users RESTART IDENTITY CASCADE")

        # –í—Å—Ç–∞–≤–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        print(f"üë• –í—Å—Ç–∞–≤–∫–∞ {len(users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        for user in users:
            cursor.execute("""
                INSERT INTO users (username, email, role, specialization, course, interests)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                user['username'], user['email'], user['role'],
                user['specialization'], user['course'], user['interests']
            ))

        # –í—Å—Ç–∞–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print(f"üìö –í—Å—Ç–∞–≤–∫–∞ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        for doc in documents:
            cursor.execute("""
                INSERT INTO documents (
                    document_id, title, authors, document_type, year, subject,
                    abstract, isbn, doi, pages, language
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                doc['document_id'], doc['title'], doc['authors'], doc['document_type'],
                doc['year'], doc['subject'], doc['abstract'], doc['isbn'],
                doc['doi'], doc['pages'], doc['language']
            ))

        # –í—Å—Ç–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
        print(f"üîç –í—Å—Ç–∞–≤–∫–∞ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
        for query in queries:
            cursor.execute("""
                INSERT INTO search_queries (query_id, user_id, query_text, results_count, timestamp, session_id)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                query['query_id'], query['user_id'], query['query_text'],
                query['results_count'], query['timestamp'], query['session_id']
            ))

        # –í—Å—Ç–∞–≤–∫–∞ –ø–æ–∫–∞–∑–æ–≤
        print(f"üëÅÔ∏è  –í—Å—Ç–∞–≤–∫–∞ {len(impressions)} –ø–æ–∫–∞–∑–æ–≤...")
        for imp in impressions:
            cursor.execute("""
                INSERT INTO impressions (
                    impression_id, query_id, user_id, document_id, 
                    query_text, position, shown_at, session_id
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                imp['impression_id'], imp['query_id'], imp['user_id'],
                imp['document_id'], imp['query_text'], imp['position'],
                imp['shown_at'], imp['session_id']
            ))

        # –í—Å—Ç–∞–≤–∫–∞ –∫–ª–∏–∫–æ–≤
        print(f"üñ±Ô∏è  –í—Å—Ç–∞–≤–∫–∞ {len(clicks)} –∫–ª–∏–∫–æ–≤...")
        for click in clicks:
            cursor.execute("""
                INSERT INTO clicks (
                    click_id, query_id, user_id, document_id, query_text,
                    position, clicked_at, dwell_time, session_id
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                click['click_id'], click['query_id'], click['user_id'],
                click['document_id'], click['query_text'], click['position'],
                click['clicked_at'], click['dwell_time'], click['session_id']
            ))

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è CTR
        print("üìä –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ CTR —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        cursor.execute("REFRESH MATERIALIZED VIEW ctr_stats")

        conn.commit()
        print("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ PostgreSQL!")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute("SELECT COUNT(*) FROM users")
        print(f"   ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {cursor.fetchone()[0]}")

        cursor.execute("SELECT COUNT(*) FROM documents")
        print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {cursor.fetchone()[0]}")

        cursor.execute("SELECT COUNT(*) FROM search_queries")
        print(f"   ‚Ä¢ –ó–∞–ø—Ä–æ—Å–æ–≤: {cursor.fetchone()[0]}")

        cursor.execute("SELECT COUNT(*) FROM impressions")
        print(f"   ‚Ä¢ –ü–æ–∫–∞–∑–æ–≤: {cursor.fetchone()[0]}")

        cursor.execute("SELECT COUNT(*) FROM clicks")
        print(f"   ‚Ä¢ –ö–ª–∏–∫–æ–≤: {cursor.fetchone()[0]}")

        cursor.close()
        conn.close()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
        raise


def save_documents_json(documents: List[Dict], filename: str = 'documents.json'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ JSON –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ OpenSearch"""
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ {filename}...")

    filepath = os.path.join(os.path.dirname(__file__), '..', 'backend', 'data', filename)
    os.makedirs(os.path.dirname(filepath), exist_ok=True)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(documents, f, ensure_ascii=False, indent=2, default=str)

    print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")


# ==================== MAIN ====================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 70)
    print("üé≤ –ì–ï–ù–ï–†–ê–¢–û–† –°–ò–ù–¢–ï–¢–ò–ß–ï–°–ö–ò–• –î–ê–ù–ù–´–• –î–õ–Ø –ë–ò–ë–õ–ò–û–¢–ï–ß–ù–û–ô –ü–û–ò–°–ö–û–í–û–ô –°–ò–°–¢–ï–ú–´")
    print("=" * 70)
    print()

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    users = generate_users(NUM_USERS)
    documents = generate_documents(NUM_DOCUMENTS)
    queries, impressions, clicks = generate_queries_and_clicks(users, documents, NUM_QUERIES)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    save_to_database(users, documents, queries, impressions, clicks)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ JSON
    save_documents_json(documents)

    print("\n" + "=" * 70)
    print("üéâ –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 70)
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(users)}")
    print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
    print(f"   ‚Ä¢ –ó–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}")
    print(f"   ‚Ä¢ –ü–æ–∫–∞–∑–æ–≤: {len(impressions)}")
    print(f"   ‚Ä¢ –ö–ª–∏–∫–æ–≤: {len(clicks)}")
    print(f"   ‚Ä¢ CTR: {len(clicks) / len(impressions) * 100:.2f}%")
    print("\n‚úÖ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ OpenSearch")
    print("   –ó–∞–ø—É—Å—Ç–∏: python scripts/load_to_opensearch.py")


if __name__ == "__main__":
    main()