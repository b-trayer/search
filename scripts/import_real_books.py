#!/usr/bin/env python3
"""
–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–º–ø–æ—Ä—Ç –∫–Ω–∏–≥ –∏–∑ JSON - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –í–°–ï –ø–æ–ª—è
"""
import json
import sys
import re
from datetime import datetime
import psycopg2
from psycopg2.extras import Json
from opensearchpy import OpenSearch, helpers

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "database": "library_search",
    "user": "library_user",
    "password": "library_password"
}

OPENSEARCH_HOST = "localhost"
OPENSEARCH_PORT = 9200
INDEX_NAME = "library_documents"

# –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏
JSON_FILE = "books.json"


def analyze_fields(records):
    """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –ø–æ–ª–µ–π –≤ JSON"""
    all_fields = {}
    
    for record in records:
        for field, value in record.items():
            if field not in all_fields:
                all_fields[field] = {
                    'count': 0,
                    'examples': [],
                    'type': type(value).__name__
                }
            all_fields[field]['count'] += 1
            if len(all_fields[field]['examples']) < 2 and value:
                all_fields[field]['examples'].append(str(value)[:50])
    
    return all_fields


def create_dynamic_table(cursor, all_fields):
    """–°–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–ª—è–º–∏"""
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Ç–∞–±–ª–∏—Ü—É documents –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    cursor.execute("DROP TABLE IF EXISTS clicks CASCADE")
    cursor.execute("DROP TABLE IF EXISTS impressions CASCADE")
    cursor.execute("DROP TABLE IF EXISTS search_queries CASCADE")
    cursor.execute("DROP TABLE IF EXISTS documents CASCADE")
    cursor.execute("DROP MATERIALIZED VIEW IF EXISTS ctr_stats CASCADE")
    
    # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
    columns = [
        "document_id VARCHAR(100) PRIMARY KEY",
        "source VARCHAR(50) DEFAULT 'e-lib.nsu.ru'",
        "indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP"
    ]
    
    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –∏–∑ JSON
    field_mapping = {}
    for field in all_fields.keys():
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–º—è –ø–æ–ª—è –≤ snake_case –¥–ª—è PostgreSQL
        pg_field = field.lower()
        pg_field = re.sub(r'[^a-z–∞-—è—ë0-9_]', '_', pg_field)
        pg_field = re.sub(r'_+', '_', pg_field).strip('_')
        
        # –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∏—Ä—É–µ–º –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if not pg_field or pg_field[0].isdigit():
            pg_field = 'field_' + pg_field
        
        field_mapping[field] = pg_field
        columns.append(f'"{pg_field}" TEXT')
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    create_sql = f"""
    CREATE TABLE documents (
        {', '.join(columns)}
    )
    """
    
    print("üìã –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã documents —Å –ø–æ–ª—è–º–∏:")
    for orig, pg in sorted(field_mapping.items()):
        print(f"   {orig} ‚Üí {pg}")
    
    cursor.execute(create_sql)
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–ª–µ–π –ø–æ–∏—Å–∫–∞
    try:
        cursor.execute('CREATE INDEX idx_doc_title ON documents("title")')
    except:
        pass
    
    return field_mapping


def recreate_related_tables(cursor):
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã"""
    
    # search_queries
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS search_queries (
            query_id SERIAL PRIMARY KEY,
            user_id INTEGER REFERENCES users(user_id) ON DELETE SET NULL,
            query_text TEXT NOT NULL,
            results_count INTEGER,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            session_id VARCHAR(100)
        )
    """)
    
    # clicks
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS clicks (
            click_id SERIAL PRIMARY KEY,
            query_id INTEGER REFERENCES search_queries(query_id) ON DELETE CASCADE,
            user_id INTEGER REFERENCES users(user_id) ON DELETE SET NULL,
            document_id VARCHAR(100),
            query_text TEXT NOT NULL,
            position INTEGER NOT NULL,
            clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            dwell_time INTEGER,
            session_id VARCHAR(100)
        )
    """)
    
    # impressions
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS impressions (
            impression_id SERIAL PRIMARY KEY,
            query_id INTEGER REFERENCES search_queries(query_id) ON DELETE CASCADE,
            user_id INTEGER REFERENCES users(user_id) ON DELETE SET NULL,
            document_id VARCHAR(100),
            query_text TEXT NOT NULL,
            position INTEGER NOT NULL,
            shown_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            session_id VARCHAR(100)
        )
    """)
    
    # ctr_stats
    cursor.execute("""
        CREATE MATERIALIZED VIEW ctr_stats AS
        SELECT 
            i.query_text,
            i.document_id,
            COUNT(DISTINCT i.impression_id) as impressions_count,
            COUNT(DISTINCT c.click_id) as clicks_count,
            CASE 
                WHEN COUNT(DISTINCT i.impression_id) > 0 
                THEN CAST(COUNT(DISTINCT c.click_id) AS FLOAT) / COUNT(DISTINCT i.impression_id)
                ELSE 0 
            END as ctr
        FROM impressions i
        LEFT JOIN clicks c ON i.query_text = c.query_text AND i.document_id = c.document_id
        GROUP BY i.query_text, i.document_id
        HAVING COUNT(DISTINCT i.impression_id) >= 3
    """)
    
    print("‚úÖ –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã")


def save_to_postgres(records, field_mapping):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ PostgreSQL"""
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(records)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ PostgreSQL...")
    
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
        field_mapping = create_dynamic_table(cursor, analyze_fields(records))
        conn.commit()
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        inserted = 0
        for i, record in enumerate(records):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
            doc_id = record.get('–ö–ª—é—á –∑–∞–ø–∏—Å–∏', '') or f"doc_{i}"
            doc_id = doc_id.replace('\\', '_').replace('/', '_')[:100]
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª—è –∏ –∑–Ω–∞—á–µ–Ω–∏—è
            fields = ['document_id', 'source']
            values = [doc_id, 'e-lib.nsu.ru']
            placeholders = ['%s', '%s']
            
            for orig_field, pg_field in field_mapping.items():
                if orig_field in record and record[orig_field]:
                    fields.append(f'"{pg_field}"')
                    values.append(str(record[orig_field])[:10000])  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
                    placeholders.append('%s')
            
            # INSERT
            sql = f"""
                INSERT INTO documents ({', '.join(fields)})
                VALUES ({', '.join(placeholders)})
                ON CONFLICT (document_id) DO NOTHING
            """
            
            try:
                cursor.execute(sql, values)
                inserted += 1
            except Exception as e:
                if inserted < 5:
                    print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")
            
            if (i + 1) % 1000 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(records)}")
                conn.commit()
        
        conn.commit()
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        recreate_related_tables(cursor)
        conn.commit()
        
        print(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ: {inserted} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
    finally:
        cursor.close()
        conn.close()
    
    return field_mapping


def create_opensearch_mapping(all_fields):
    """–°–æ–∑–¥–∞—Ç—å –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è OpenSearch"""
    
    properties = {
        "document_id": {"type": "keyword"},
        "source": {"type": "keyword"},
        "indexed_at": {"type": "date"}
    }
    
    # –ü–æ–ª—è –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
    text_fields = ['title', '–∞–≤—Ç–æ—Ä—ã', '–¥—Ä—É–≥–∏–µ –∞–≤—Ç–æ—Ä—ã', 'abstract', '–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è', 
                   '–æ–ø–∏—Å–∞–Ω–∏–µ', '–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è']
    
    # –ü–æ–ª—è-–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    keyword_fields = ['url', 'cover', '—è–∑—ã–∫', '—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', 
                      '—É–¥–∫', '–±–±–∫', '–ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞', '–∫–ª—é—á –∑–∞–ø–∏—Å–∏']
    
    for field in all_fields.keys():
        field_lower = field.lower()
        pg_field = re.sub(r'[^a-z–∞-—è—ë0-9_]', '_', field_lower)
        pg_field = re.sub(r'_+', '_', pg_field).strip('_')
        
        if any(x in field_lower for x in text_fields):
            properties[pg_field] = {
                "type": "text",
                "analyzer": "russian_custom",
                "fields": {"keyword": {"type": "keyword"}}
            }
        else:
            properties[pg_field] = {"type": "text", "analyzer": "russian_custom"}
    
    return properties


def save_to_opensearch(records, all_fields):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ OpenSearch"""
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ {len(records)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ OpenSearch...")
    
    client = OpenSearch(
        hosts=[{'host': OPENSEARCH_HOST, 'port': OPENSEARCH_PORT}],
        http_compress=True,
        use_ssl=False,
        verify_certs=False
    )
    
    info = client.info()
    print(f"   –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ OpenSearch {info['version']['number']}")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å
    if client.indices.exists(index=INDEX_NAME):
        client.indices.delete(index=INDEX_NAME)
        print(f"   –°—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å —É–¥–∞–ª—ë–Ω")
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –º–∞–ø–ø–∏–Ω–≥–æ–º
    index_settings = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "russian_custom": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "filter": ["lowercase", "russian_stop", "russian_stemmer"]
                    }
                },
                "filter": {
                    "russian_stop": {"type": "stop", "stopwords": "_russian_"},
                    "russian_stemmer": {"type": "stemmer", "language": "russian"}
                }
            }
        },
        "mappings": {
            "dynamic": True,  # –†–∞–∑—Ä–µ—à–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
            "properties": create_opensearch_mapping(all_fields)
        }
    }
    
    client.indices.create(index=INDEX_NAME, body=index_settings)
    print(f"   –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    actions = []
    for i, record in enumerate(records):
        doc_id = record.get('–ö–ª—é—á –∑–∞–ø–∏—Å–∏', '') or f"doc_{i}"
        doc_id = doc_id.replace('\\', '_').replace('/', '_')[:100]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –ø–æ–ª—è
        doc = {
            "document_id": doc_id,
            "source": "e-lib.nsu.ru",
            "indexed_at": datetime.now().isoformat()
        }
        
        for field, value in record.items():
            if value:
                pg_field = re.sub(r'[^a-z–∞-—è—ë0-9_]', '_', field.lower())
                pg_field = re.sub(r'_+', '_', pg_field).strip('_')
                doc[pg_field] = str(value)
        
        actions.append({
            "_index": INDEX_NAME,
            "_id": doc_id,
            "_source": doc
        })
    
    # Bulk –∑–∞–≥—Ä—É–∑–∫–∞
    success, failed = helpers.bulk(client, actions, chunk_size=500, raise_on_error=False)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {success} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    if failed:
        print(f"‚ö†Ô∏è  –û—à–∏–±–æ–∫: {len(failed)}")
    
    client.indices.refresh(index=INDEX_NAME)


def main():
    print("=" * 60)
    print("üìö –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ò–ú–ü–û–†–¢ –ö–ù–ò–ì –ò–ó JSON")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ JSON
    print(f"\nüìñ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {JSON_FILE}")
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f:
            records = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {JSON_FILE}")
        sys.exit(1)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(records)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–µ–π
    print("\nüîç –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–µ–π...")
    all_fields = analyze_fields(records)
    
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(all_fields)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π:")
    for field, info in sorted(all_fields.items(), key=lambda x: -x[1]['count']):
        print(f"   {field}: {info['count']} –∑–∞–ø–∏—Å–µ–π")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    field_mapping = save_to_postgres(records, all_fields)
    save_to_opensearch(records, all_fields)
    
    print("\n" + "=" * 60)
    print("‚úÖ –ò–ú–ü–û–†–¢ –ó–ê–í–ï–†–®–Å–ù!")
    print("=" * 60)
    print(f"\nüîç –¢–µ—Å—Ç:")
    print(f"   curl -X POST http://localhost:8000/api/search/ \\")
    print(f"     -H 'Content-Type: application/json' \\")
    print(f"     -d '{{\"query\": \"–∏—Å—Ç–æ—Ä–∏—è\", \"top_k\": 5}}'")


if __name__ == "__main__":
    main()
